---
title: "Platform"
description: "We describe the most common platform requirements for successful synthetic data adoption. "
---

### **A hosted SaaS service is a non starter** 

By design, this product is intended to run on-premises, including in air-gapped environments. A SaaS-hosted platform, where enterprises upload their data to train models on our servers, defeats the core purpose of the product. Its very reason for existence is to ensure sensitive enterprise data never leaves organizational boundaries—not when working with third parties, and not even for internal use by developers. As such, any suggestion to upload data externally is a non-starter. 

### Enterprise data requierments

To succeed with wider adoption of synthetic data within an entperise these are the requirements that can act as a guide when evaluating platforms.

| Scalability               | A synthetic data platform [<u>must be able to model 100s of interconnected tables</u>](https://datacebo.com/blog/multi-table-synthesizers/)  in order to be useful for enterprise environments.                                                           |
| ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Address hidden context    | Much of the complexity comes from hidden context not captured in schemas or metadata. [<u>These represent hard constraints</u>](https://datacebo.com/announcements/introducing-cag/) that the platform must recognize and enforce to generate valid data. |
| Generate flexible formats | Generative models require properly prepared inputs, typically numerical. Because enterprise data spans diverse formats, the platform must include a robust transformation toolkit that supports forward and reverse transforms.                           |

### Model flexibility

A platform must provide ability to model data of various types (multi table, sequential and single table), but it must also provide multiple models that are good along different axes. Different use cases require different levels of speed, quality, privacy and transparency. Thus for each modality a platform must provide a prularity o models. Here are few examples why that is needed. 

**Creating synthetic data for clinical trials.**\
Imagine you have data from only a small number of clinical trials and need to augment it with additional synthetic samples. In this context, transparency is critical. Models built using classical statistical techniques often provide greater explainability and traceability than complex generative methods, making them better suited for regulated environments where auditability is essential.

**Creating synthetic data for consumer surveys.**\
Consumer survey datasets are often extremely limited in size. With such scarcity, GAN-based approaches are prone to mode collapse and may fail to generate meaningful variation. Because collecting more survey data defeats the very purpose—deriving insights with fewer samples and supplementing them synthetically—you need an approach designed to operate reliably under severe data constraints.

**Creating synthetic data when real data consists of only hundreds of samples.**\
In some scenarios, each data point is expensive to obtain—for example, when every observation requires an in-person session in a controlled laboratory environment. You may end up with only a few hundred records across many dimensions. This high-dimensional, low-sample-size problem requires a specialized synthesizer that can model complex relationships without overfitting or collapsing.

| Model plurality    | A platform must provide multiple modeling choices for each modality of data. Theses models must allow trade-offs between quality, speed, privacy and transparency. |
| :----------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Specialized models | A platform must provide ability to train models that can train from as little data possible and also enable training models for highly segmented data.             |

### Computational requirements

| Non GPU based algorithms | The platform must enable algorithms that run on CPUs. GPU dependency increases cost and complexity, limiting the number of viable use cases. |
| :----------------------- | :------------------------------------------------------------------------------------------------------------------------------------------- |
|                          |                                                                                                                                              |
|                          |                                                                                                                                              |

### Evaluation

- **ROI vs. Quality:** Evaluation should focus on ROI metrics rather than only on synthetic data quality metrics. Synthetic data quality does not always correlate linearly with ROI. In some cases, higher “quality” does not translate into greater business value for a given use case.
- **Flexibility for Use Cases:** A powerful synthetic data platform must integrate multiple layers to support diverse use cases, each with distinct quality requirements.
- **Performance Metrics:** When benchmarking, both the model training time and the time required to generate synthetic data should be considered critical evaluation metrics.
- **Robust Evaluation Metrics:** Evaluation should rely only on openly validated, peer-reviewed metrics from academia and industry. We created [**<u>SDMetrics </u>**](https://docs.sdv.dev/sdmetrics)for this purpose, and it has become widely adopted across both domains.

### Benchmarking

- **Benchmarking Tools:** To enable consistent assessment, we created the **SDGym library**, which provides proper abstractions to evaluate synthetic data along two axes, data quality and performance time.
- \_In this graphic, we are showing how different synthetic data generators perform along the time-quality axes. The line represents the pareto front and the ones on the line are good models that have good tradeoffs. Such evaluations are important as users may choose synthesizers based on their need. Some use cases require rapid iteration. 

### Privacy

A synthetic data platform should support the sharing and portability of the synthesizer (model) itself, not just the generated data. As a result, privacy-preservation must be evaluated not only on the output data but also on the synthesizer. Without the ability to securely share synthesizers, several important use cases are limited.

Our launch of **differentially private synthesizers** demonstrates this capability:

- **Part 1**[<u>introduces the framework</u>](https://datacebo.com/blog/differential-privacy/)  for creating differentially private synthesizers.
- **Part 2** [_<u>introduces our trust-but-verify framework</u>_](https://datacebo.com/blog/differential-privacy-2/), which enables independent verification of the privacy guarantees for a synthesizer.

### Use case support