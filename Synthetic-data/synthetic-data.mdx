---
title: "What is synthetic data ? "
description: "This page provides the background on synthetic data and how it evolved over decades "
---

Synthetic data is data generated by an AI model (commonly known as a generative AI model). To create synthetic data, you first train a generative AI model using real data. Once a model is created, you can sample data from the model that looks real. 

### Types of data: images, text, and tabular 

Today, generative AI is highly developed, and can be used to generate a variety of types of synthetic data, including images, videos, audio data, text data, and tabular data. One of the biggest innovations that jump-started the field of synthetic data occurred in 2014 and 2015, when the development of generative adversarial networks led to the creation of synthetic images. Some of these images look like photographs of real people, but the people don't actually exist — the photographs are created by models and algorithms. The newest generation of generative AI models allows users to provide a prompt or define a demographic and receive synthetic images that look very, very real, even with just a desktop computer. [_Faces images here_]

Another well-known type of synthetic data generator is the large language model, or LLM.  LLMs are the engines underneath ChatGPT. Although ChatGPT outputs clear, human-like representations of natural language, a human is not actually writing them. ChatGPT's answers are LLM-generated synthetic text data.

The third type of data that can be synthesized, and the most common and important for most enterprises, is tabular data. This includes relational data, or any kind of data collected from customers or consumers as they interact with products and services. It's now possible to create synthetic data for tabular data as well. Of the two data tables shown below, can you pick out which one is real, and which one is synthetic? 

   

\_Answer: The left table is real data, and the right is synthetic. While they look very similar, it's possible to spot some differences — and, if you know the domain well, to identify which is which. \_

There's one big difference between image or language data and tabular data: Image data and language data are easy to assess. Anyone can look at a collection of this type of data and determine whether or not it looks real, is reasonable, and makes sense. However, it's much harder for an average person to judge tabular data. You have to know the domain very well, and know things like column names, in order to determine whether synthetic tabular data is realistic. That's one of the challenges inherent in assessing synthetic tabular data, which we'll elaborate on later.

## How is synthetic data useful? 

The first advantage of synthetic data is that it can stand in for real data, helping to mitigate data access issues due to security and privacy concerns. This continues to be one of the biggest reasons to use synthetic data. If you create a model and sample data with all the statistical properties of real data, you can use this synthetic data in place of the real data. You can give it to team members or third parties without worrying about security or privacy, allowing you to scale up your productivity. In 2016, a team from MIT published a peer-reviewed study showing that analysis performed by data scientists on synthetic data achieved the same results as analysis performed by data scientists on the equivalent real data.

The second interesting advantage of synthetic data is increased data volume. Because the model itself is unlimited in terms of how many samples you can draw from it, it allows you to create a lot of data for testing purposes, even if you started with very little — if you have data from ten or fifteen or a hundred customers, and you learn a model from it, you can sample synthetic data as though you have data from a thousand or a million customers instead. In addition, sometimes you can create targeted data, focusing on specific demographics. If you tell the model, '_Give me a lot of users from Ohio between the ages of 20 and 30,_' the model will specify those two conditions and will generate the rest of the variables for you. 

\
The flip side of this is also interesting. Let's say instead of too little data, you have terrabytes and terrabytes of data — perhaps 50 million customers' or patients' data. When you learn the generative AI model, the model itself is smaller — only a couple of gigabytes. This is also a powerful advantage, because it essentially gives you data portability: You can ship the model around inside the company safely, and whoever has it can sample as much data as they want.

Synthetic data also allows for data quality checks. A model is a representation of the data and the underlying process that generated it, so when you have new data incoming, you can check that data against the model. The model can determine whether it has seen this kind of data before — and if it hasn't seen it, it will flag it, saying '_This data coming in is very different than anything I've seen before._' This helps assess data quality before using it for downstream use cases. \
\
Similarly, you can use the model to create only representative data points — making it possible to create and send a small set of representative data points, even if there was a lot of data in the original database. 

## **The background of synthetic data generation, and how methods have evolved over the years… and even the decades.** 

Synthetic data generation has been happening for longer than you may think. The first methods were rule-based — in other words, engineers would write specific rules for how to generate the data. For instance, in the healthcare domain, where people deal with a lot of patient data, there is a rule-based tool called Synthea, by MITRE. Then there are statistical techniques, some of which date back to the 1950s. More recently, data scientists have used neural networks for this purpose. And we expect many more tools in the future — the number of techniques that are being produced are growing every day, and there are more coming out as we speak.

### Copulas 

One important statistical technique is called _copulas_. Although it's an older technique, from 1959 or even earlier, it's very effective — in fact, many of the latest techniques in the machine learning community struggle to beat it. Below you can see two data sets, represented as 3D graphs, with each point standing in for a data point. On the left side is real data, and on the right side is synthetic data. Copulas attempt to capture the distribution of each one of the columns, as well as the correlations between multiple variables. This is done by learning a univariate distribution, and then a multivariate distribution. Once you've built the copulas model, it captures all those properties, and you can sample data from the model. Below on the right is synthetic data that has been sampled, and you can see that the shape of the real data (on the left) is preserved. Copulas is not only one of the easiest techniques to use — it's also one of the most powerful techniques yet found, even after all these years. For tabular data specifically, no other model has been able to beat copulas in terms of performance, accuracy and fidelity. 

### **Long Short-Term Memory Recurrent Neural Networks.** 

Another synthetic data generation technique is called _long short-term memory recurrent neural networks_. This technique was developed in order to learn from time series or sequential data. In sequential data, any one time point is dependent on the previous time point. Say you have a doctor's visit, and then after the visit you go to the pharmacy to get some medicine – you couldn't possibly do these in reverse, because you wouldn't know what medicine you need. There are sequential aspects to the data and how it's generated. 

Recurrent neural networks are popularly used to model this kind of sequential data, where one data point depends on preceding ones. Recurrent neural networks assume that there is a hidden state that determines the flow of information or factual data creation. Even if we don't know that state, it's there, determining how things are generated. With recurrent neural networks, you learn a model based on this assumption, and you in put data into the model representing that state. Then you get the output, and you feed it back to the next step and the next state. By continuing on like that, you generate the data in sequence. 

Sequential models are very good for sequential data. If you have a sequence of events, and you think that the next event is going to depend on the previous event, this is the kind of generative model you want to use.

### **Generative adversarial networks brought the breakthrough.** 

The biggest breakthrough in synthetic data in the last decade came from _generative adversarial networks_ or GANs. GANs led to synthetic image generation, as described earlier. GANs involves two neural networks — standard AI techniques. You ask one neural network, called a generator, to generate realistic-looking data. And then you ask the other neural network, called the discriminator, to detect whether a particular data point is _real_ or _synthetic_. Both networks compete with each other (thus _adversarial_). As a result, both networks eventually achieve an equilibrium, where the generator gets so good at producing synthetic images that the discriminator is not able to discriminate. All of this is done in software. 

This innovation occurred around 2014 or 2015. It spurred a new wave of synthetic data generation, allowing people to create high-resolution images that were hard to tell apart from real photos. A team at MIT has adapted GANs to work for tabular data, introducing a method called CTGAN, which is now widely used to generate tabular synthetic data. 

### **Generative pretrained transformers.** 

Last but not least are generative pretrained transformers, which have revolutionized synthetic language data and continue to power ChatGPT and other language generators. So there's a new technique called attention-based (?), and they created that. The most interesting thing about this technique is that it allows models to learn from a large corpus of language. It takes in huge amounts of this language data, and is able to learn a pre-trained model. That's why ChatGPT is able to do what it does — because so much language data from around the world is available on the Internet. 

Once you have a pretrained model like this, you can use it for many purposes — for instance, you can ask it questions, as with ChatGPT. It's difficult to approach a lot of tabular data, including health care data, in this way — because it's generally not publicly available at the scale at which language data is available, and thus it's not possible to learn a pretrained model on it. But this technique works very well for text data, and has spurred the rise of ChatGPT and similar tools. 

**GPT-3**