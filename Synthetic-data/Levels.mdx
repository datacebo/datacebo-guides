---
title: "Various types and levels of synthetic data generation "
description: "Categorizing synthetic data into various levels and their use. "
---

Synthetic data for relational and tabular datasets has come a long way. Technically the definition of synthetic data is data generated using algorithms. These algorithms can be written by developers, or automatically created by machine learning, or generative AI tools specifically large language models (LLMs for short).  However, there are multiple types of synthetic data and each type can support different level of use. In this guide we will define:

- Types of synthetic data. Types are organized by the expected quality and use of the data produced and the properties it has.
- Levels of synthetic data generation.  Levels of generation organizes different ways of generating synthetic data based on the manual effort required to get synthetic data that can be used for different applications. 

## Types of synthetic data 

| Type | Defining property                                       |
| ---- | ------------------------------------------------------- |
| 1    | No information content synthetic data                   |
| 2    | Heuristically corrected synthetic data                  |
| 3    | Information rich synthetic data                         |
| 4    | Information rich conformative synthetic data            |
| 5    | Infinitely information rich conformative synthetic data |

**Type 1: No information content synthetic data**. Data has no information as it is completely randomly created. In many cases, most data is uniformly distributed, no correlations are present. 

**Type 2: Heuristically corrected synthetic data.** Data here is step up from Type 1 with some patterns (information content) introduced by patching some of the data generated in Type 1, either manually or using a tool. For example, changing the distribution of gender column to 70% male and 30% female. 

**Type 3: Information rich synthetic data.** Data here follows all the statistical distributions and has the correlations that your real data may have. It does not require any correction as it is able to capture those properties. However, some post processing is required to adjust the formats, and making it conform to your data structures - relational structure, referential integrity and other important constraints. 

**Type 4: Information rich conformative synthetic data.** Data here has all the statistical properties and also conforms to your original structure - whether there it is the relational structure or constraints. Data here is ready to use without any post processing. It emulates your real data completely and can be directly embedded into downstream applications. 

**Type 5: Infinitely information rich conformative synthetic data.** Data here has richness beyond what you see in your real data. It contains diverse data in the neighborhood of your original data. For example, if your original data only has customers between age of 19-36 buying a product A, and you have customers between the age of 26-36 who bought both product A and product B, it has data that has synthetic customers buying product B between the ages of 19-26. 

<Check>
  At Type 4 where the data is information rich and is conformative to your original structures you are able to use the data directly for your downstream applications. Lack of ability to produce synthetic data at this level is a biggest blocker for synthetic data adoption and reduces the scale at which you can use synthetic data - for one experimental project vs. at an enterprise level. 
</Check>

## Levels of synthetic data generation 

Before we categorize different synthetic data generation methods, let us go through different methods of generating synthetic data and the workflows that they entail. For each method, we will provide what type of synthetic data it produces based on our type classification above and also describe methods and techniques used to bring them to type 4 at which point the synthetic data becomes usable synthetic data. 

### Methods of synthetic data generation 

**Mock data generation**. Methods to generate mock data have existed for a long time. In this one can generate data by writing wrappers around faker or create homegrown scripts or use some tooling provided by the synthetic data vendor - where the vendor has on top of faker has provided you tooling to generate data that is conforms to your formats, creates referentially sound data. To generate this data you don't require any real data and you can generate using metadata alone. The generated data qualifies as type 1 since it has no information, it is completely random. To transform mock data to Type 4, you would require a huge amount of manual effort and it is not typically done. Instead many of our cusomters attempt to use other solutions, instead of attempting to fix the mock data and make it look realistic. 

<Note>
  A synthetic data vendor may provide tooling to generate mock data. The value add with the vendor tooling here is that their tool can produce data that conforms to your formats and if the data is relational it will ensure referential integrity. But essentially the data is still type 1, it has no information content. 
</Note>

**Rule based data generation**. In this method, one would write specific rules to generate the data. The rules come from the domain knowledge, or knowing some distributions from the real data. One may over time develop a software system that has all the rules that generates this data. Often these rules need to be maintained, updated. The data generated here would be type 2 since many rules are heuristic representation of what may have been seen in the real data. It has some information content since some distributions have been specified. It will conform to the formats and structures. However, to get it to Type 4 would require a lot of manual work. Identifying all the patterns that exist in real data and translating into rules is next to impossible with a human effort. 

**Using LLMs to generate data**. Recently, there has been a surge of interest in creating tabular data using LLMs. In most cases, the usage of LLMs is to not have to code in any programming language to generate the data like the previous two methods or use a user interface, but to be able to directly ask LLMs to generate the data using natural language prompts. This is attractive option if one wants to produce mock data or specify rules to generate data without having to code or without needing any software or tooling. You can use metadata to generate. If you have metadata, it works in three steps: 

<Steps>
  <Step title="Generate data by providing metadata to LLMs">
    As a first step you provide the metadata to the LLM and ask it to generate the data. You can also ask it to provide the code that you can use in the future to generate the data. If the data is satisfactory you can simply download the data and use it. The data generally created by LLMs is Type 1 or if it makes some approximations for distributions of certain columns - say age,  it will be Type 2. 
  </Step>
  <Step title="Prompt it with rules to refine the data ">
    After your initial dataset, you can look at the data and provide it additional rules. These additional rules can be provided as prompts. For example, you can say, make sure 80% of my account types are premium and 20% are standard. You can provide a number of rules like this. You may glean information like this from your real data and provide this to the LLMs. 
  </Step>
  <Step title="Download the data ">
    Once you are fully satisfied with the data you can download this data.
  </Step>
</Steps>

Data generated here is either Type 1 or Type 2 as it is heuristically corrected via rules and prompts. To generate data that is Type 4 and usable for downstream applications, one would need a lot of manual effort to create rules for different columns. Even so, humans can specify distributions for some columns, but may not be able to specify correlations between the columns. 

<Note>
  A synthetic data vendor may provide tooling to enable you to do these three steps by providing an interface to an LLM. The tool may ease your interaction with the LLM, may provide some guardrails to make sure an LLM reliably follows your instructions, may provide ability to specify rules via simple config files instead of prompts. However, despite all that you are still generating data that is Type 2 and it would require a huge amount of manual effort to identify all the rules from your real data and provide it to the tool. 
</Note>

**Building your own generative model**. An LLM is a generative model trained on the world's corpus of data. Similarly, you can train a generative model on your own specific data if you have access to training software. In this case with a software that can train a generative model you can do the following: 

<Steps>
  <Step title="Train a generative model by providing metadata and real data ">
    
  </Step>
  <Step title="Sample synthetic data from the generative model ">
    
  </Step>
  <Step title="Post process and use the data ">
    
  </Step>
</Steps>

Data generated this way will emulate the statistical properties in your real data, it will be rich in information. It will capture all the salient patterns seen in your real data and emulate that in synthetic data (without repeating your real data points). To get it to Type 4, you may have to do some post processing. For some simpler datasets, you would not need to do any post processing and the data is already type 4. 

<Note>
  A synthetic data vendor will provide you a software system to build your own generative model and sample from it. You can assess the vendor from the point of view of how much real data do they need to create the generative model, computational effort needed to create the model, how much pre and post processing they expect you to do when creating the model and sampling the data. However, at this level you are creating synthetic data that is at Type 3 and possibly at Type 4 depending upon the complexity in your original data. 
</Note>

**Building constraints augmented generative model** 

### Summary 

| Method                                          | Type of data generated | Manual effort needed | Real data needed |
| ----------------------------------------------- | ---------------------- | -------------------- | ---------------- |
| Mock data generation                            | Type 1                 | Very high            | No               |
| Rule based data generation                      | Type 2                 | Very high            | No               |
| Using LLMs to generate data                     | Type 1 / Type 2        | High                 | Yes              |
| Building your own foundation model              | Type 3                 | Low                  | Yes              |
| Building constraints augmented foundation model | Type 4                 | Very Low             | Yes              |